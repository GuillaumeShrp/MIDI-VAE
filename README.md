# MIDI-VAE (Modified version personal project in 2022)


## Exploring Strategies for Latent Space Disentanglement of Variational AutoEncoders in Musical Style Transfer

Neural style transfer applied to music is the task for a machine that consists in
changing styles of an art piece. From the late 2010s, dozens of relevant research papers
have developed models to perform such a task. This is a key technological advancement
to boost creativity into the computer artwork but introducing artificial intelligence
creation. These creations are still conditioned by the datasets used to train such
machine learning algorithms. The computer graphic area has already seen successful
style transfer performance, opening new art perspectives.
This project focuses on the application of machine learning creativity to the musical
field. More precisely, about the rework of already existing piece of music into another
musical style than its original.


The main goal of this project is directly inspired by the ending observation made
in one of the latest articles dealing with the subject, namely about the network
Clavinet. Significant results were achieved, especially about designing successful
network for VAE latent space. However, disentanglement mechanism within the models
still remains to be studied. In this work, we explore a new approach to redesign the
latent space in order to carry on Clavinet project. In this attempt, a new architecture
of latent space is designed with less stochasticity. The intuitive idea is that musical
styles follow strict rules and one a few parameters remain for randomness. By such a
simplification the new design is proved to be a failure. This final result is a significant
conclusion towards this new approach and the next latent space approach for future
innovation in musical applications of AI. 

## Original Paper

[MIDI-VAE: MODELING DYNAMICS AND INSTRUMENTATION OF
MUSIC WITH APPLICATIONS TO STYLE TRANSFER](https://www.tik.ee.ethz.ch/file/b17f34f911d0ecdb66bfc41af9cdf200/MIDIVAE_ISMIR_CR.pdf)

Paper accepted at 19th International Society for Music Information Retrieval Conference (ISMIR), Paris, France, September 2018
